import os
import requests
import asyncio
import json
from typing import List, Dict, Any
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent
from dotenv import load_dotenv

from dify2 import dify_retrieve

# åŠ è½½.envæ–‡ä»¶
load_dotenv()

# ä»ŽçŽ¯å¢ƒå˜é‡è¯»å–é…ç½®
OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY')
OPENROUTER_BASE_URL = os.getenv('OPENROUTER_BASE_URL', 'https://openrouter.ai/api/v1')

async def stream_with_token_output():
    """ä½¿ç”¨ astream_events å®žçŽ°é€ token æµå¼è¾“å‡º"""
    
    # åˆ›å»ºLLMå®žä¾‹ï¼Œå¿…é¡»è®¾ç½® streaming=True
    llm = ChatOpenAI(
        model="google/gemini-2.5-flash",
        api_key=OPENROUTER_API_KEY,
        base_url=OPENROUTER_BASE_URL,
        temperature=0.7,
        streaming=True  # å…³é”®ï¼šå¯ç”¨æµå¼è¾“å‡º
    )
    
    # åˆ›å»ºReAct Agent
    agent = create_react_agent(
        model=llm,
        tools=[dify_retrieve],
        prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·æ¥æ£€ç´¢ä¿¡æ¯å¹¶å›žç­”ç”¨æˆ·é—®é¢˜ã€‚è¯·åŸºäºŽæ£€ç´¢åˆ°çš„ä¿¡æ¯ç»™å‡ºè¯¦ç»†ã€æœ‰ç”¨çš„å›žç­”ã€‚"
    )
    
    test_query = "è¯·å¸®æˆ‘æŸ¥è¯¢ç”Ÿæ€å†œä¸šç›¸å…³çš„ä¿¡æ¯"
    print(f"ðŸ¤– å‘Agentå‘é€æŸ¥è¯¢: {test_query}")
    print("=" * 60)
    
    # ä½¿ç”¨ astream_events èŽ·å–æ¯ä¸ªäº‹ä»¶
    async for event in agent.astream_events(
        {"messages": [{"role": "user", "content": test_query}]},
        version="v1"  # ä½¿ç”¨ v1 ç‰ˆæœ¬çš„ API
    ):
        event_type = event.get("event")
        event_name = event.get("name", "")
        
        # æ•èŽ· LLM å¼€å§‹æ€è€ƒ
        if event_type == "on_chat_model_start":
            print(f"\nðŸ§  [{event_name}] å¼€å§‹æ€è€ƒ...")
            
        # æ•èŽ·æ¯ä¸ª token çš„è¾“å‡º
        elif event_type == "on_chat_model_stream":
            chunk = event.get("data", {}).get("chunk", {})
            if hasattr(chunk, 'content') and chunk.content:
                print(chunk.content, end="", flush=True)
                
        # æ•èŽ· LLM å®Œæˆæ€è€ƒ
        elif event_type == "on_chat_model_end":
            print(f"\nâœ… [{event_name}] æ€è€ƒå®Œæˆ")
            
        # æ•èŽ·å·¥å…·è°ƒç”¨å¼€å§‹
        elif event_type == "on_tool_start":
            tool_name = event.get("name", "")
            tool_input = event.get("data", {}).get("input", {})
            print(f"\nðŸ”§ [{tool_name}] å¼€å§‹æ‰§è¡Œå·¥å…·")
            print(f"   å‚æ•°: {json.dumps(tool_input, ensure_ascii=False)}")
            
        # æ•èŽ·å·¥å…·æ‰§è¡Œç»“æžœ
        elif event_type == "on_tool_end":
            tool_name = event.get("name", "")
            tool_output = event.get("data", {}).get("output", "")
            print(f"\nâš¡ [{tool_name}] å·¥å…·æ‰§è¡Œå®Œæˆ")
            
            # æ­£ç¡®å¤„ç†ä¸åŒç±»åž‹çš„è¾“å‡º
            if hasattr(tool_output, 'content'):
                output_content = tool_output.content
            elif isinstance(tool_output, str):
                output_content = tool_output
            else:
                output_content = str(tool_output)
                
            # æ˜¾ç¤ºç»“æžœçš„å‰100ä¸ªå­—ç¬¦
            if len(output_content) > 100:
                print(f"   ç»“æžœ: {output_content[:100]}...")
            else:
                print(f"   ç»“æžœ: {output_content}")

if __name__ == "__main__":
    asyncio.run(stream_with_token_output())